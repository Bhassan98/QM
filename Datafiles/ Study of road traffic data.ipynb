{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Study of road traffic data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### The Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Outlined here are the what each of the 31 columns represent\n",
    "\n",
    " * Light_Conditions: The illumination conditions prevalent at the site where the accident occurred.\n",
    "\n",
    " * Road_Type: The classification describing the type of road where the accident occurred, which can include roundabouts, one-way streets, dual carriageways, or single carriageways.\n",
    "\n",
    " * Date: The calendar date when the accident occurred.\n",
    "\n",
    " * 2nd_Road_Class: A classification indicating the category of the secondary road involved in the accident.\n",
    "\n",
    " * 1st_Road_Class: A classification signifying the category of the primary road involved in the accident, encompassing highways, A roads, B roads, or other classifications.\n",
    "\n",
    " * Road_Surface_Conditions: The state of the road surface, encompassing factors like wet, icy, or dry conditions.\n",
    "\n",
    " * Location_Easting_OSGR: The eastward positional coordinate of the accident site, referenced to the Ordnance Survey Grid.\n",
    "\n",
    " * Urban_or_Rural_Area: An indication of whether the accident transpired in an urban or rural region.\n",
    "\n",
    " * Speed_limit: The legal speed limit imposed on the road segment where the accident transpired.\n",
    "\n",
    " * Police_Force: The law enforcement agency responsible for responding to and managing the accident.\n",
    "\n",
    " * Location_Northing_OSGR: The northward positional coordinate of the accident site, referenced to the Ordnance Survey Grid.\n",
    "\n",
    " * Number_of_Vehicles: The count of automobiles involved in the accident.\n",
    "\n",
    " * Day_of_Week: The specific day of the week when the accident took place, represented numerically (1 for Sunday, 2 for Monday, etc.).\n",
    "\n",
    " * Weather_Conditions: The atmospheric conditions, such as rain, snow, or fog, prevailing at the accident location.\n",
    "\n",
    " * 2nd_Road_Number: The numerical identifier assigned to the secondary road involved in the accident.\n",
    "\n",
    " * Pedestrian_Crossing-Human_Control: The type of human control employed at pedestrian crosswalks located at or near the accident scene.\n",
    "\n",
    " * Junction_Control: The mode of control governing the junction or intersection where the accident transpired.\n",
    "\n",
    " * Longitude: The horizontal geographical coordinate pinpointing the accident's position.\n",
    "\n",
    " * Number_of_Casualties: The tally of individuals who sustained injuries or fatalities as a result of the accident.\n",
    "\n",
    " * Pedestrian_Crossing-Physical_Facilities: The type of physical infrastructure available at pedestrian crosswalks situated at or near the accident location.\n",
    "\n",
    " * Local_Authority_(District): The governing body overseeing the administrative district where the accident transpired.\n",
    "\n",
    " * Special_Conditions_at_Site: Unusual or specific circumstances present at the accident site.\n",
    "\n",
    " * 1st_Road_Number: The numerical identifier assigned to the primary road involved in the accident.\n",
    "\n",
    " * Junction_Detail: A detailed description of the junction or intersection at which the accident occurred.\n",
    "\n",
    " * Latitude: The vertical geographical coordinate pinpointing the accident's position.\n",
    "\n",
    " * Local_Authority_(Highway): The governing body responsible for maintaining and managing the road on which the accident occurred.\n",
    "\n",
    " * Did_Police_Officer_Attend_Scene_of_Accident: A binary indicator revealing whether law enforcement personnel attended the accident scene.\n",
    "\n",
    " * Accident_Index: A unique identifier assigned to each accident incident.\n",
    "\n",
    " * Accident_Severity: A measure indicating the extent of seriousness of the accident, reflecting its impact and outcomes.\n",
    "\n",
    " * Time: The precise moment when the accident took place.\n",
    "\n",
    " * Carriageway_Hazards: Hazards or dangers on the roadway at the accident location.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Exploring the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Before proceeding, our initial step involves exploring the structure and assessing the overall data quality of our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import plotly.express as px\n",
    "from libpysal.weights import Kernel\n",
    "from esda.moran import Moran\n",
    "from esda.moran import Moran_Local\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from scipy.stats import chi2_contingency\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([ pd.read_csv(\"/Users/bashir/Documents/UCL/Quantitative Methods/Written Investigation/Datafiles/accidents_2005_to_2007.csv\") ,\n",
    "                         pd.read_csv(\"/Users/bashir/Documents/UCL/Quantitative Methods/Written Investigation/Datafiles/accidents_2009_to_2011.csv\"), \n",
    "                         pd.read_csv(\"/Users/bashir/Documents/UCL/Quantitative Methods/Written Investigation/Datafiles/accidents_2012_to_2014.csv\")], axis=0, ignore_index=True)\n",
    "\n",
    "\n",
    "df=df.reset_index(drop=True)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " One of the initial observations I made is that the Index column appears to contain duplicate values. This may pose a potential issue for us down the line, as we require a unique identifier to serve as an index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [\"Accident_Index\",[\"Longitude\",\"Latitude\"],\"Latitude\",\"Longitude\",\"Date\",\"Time\",[\"Date\",\"Time\"],[\"Date\",\"Time\",\"Latitude\",\"Longitude\"]]:\n",
    "    print(i,' ',len(df.drop_duplicates(subset=i)))\n",
    "\n",
    "print(len(df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coefficient_of_variation(df):\n",
    "    cv_data = {}\n",
    "    for col in df.columns:\n",
    "        if pd.api.types.is_numeric_dtype(df[col]):\n",
    "            col_data = df[col].dropna()  \n",
    "            if len(col_data) > 0:  \n",
    "                col_cv = col_data.std() / col_data.mean() \n",
    "                cv_data[col] = col_cv\n",
    "\n",
    "    cv_df = pd.DataFrame(cv_data, index=['coefficient_of_variation']).T\n",
    "    return cv_df\n",
    "\n",
    "coefficient_of_variation_df = coefficient_of_variation(df)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(coefficient_of_variation_df, annot=True, cmap=\"YlGnBu\")\n",
    "plt.title(\"Heatmap of Coefficient of Variation\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * 'Number_of_Vehicles' has a moderate CV of 0.39, indicating a moderate level of spread or variability in the data.\n",
    "\n",
    " * 'Number_of_Casualties' exhibits a higher CV of 0.611, indicating that the values are more widely distributed compared to 'Number_of_Vehicles.'\n",
    "\n",
    " * 'Longitude' has a negative CV of -0.973, which is unusual. This suggests that the standard deviation of longitude values is greater than the mean. This could be attributed to the fact that longitude values are typically negative, causing the mean to be negative, while the standard deviation remains positive.\n",
    "\n",
    " * 'Latitude' shows a very low CV of 0.028, indicating that the values are very close to the mean.\n",
    "\n",
    " These CV findings provide valuable insights into the dispersion of data in our dataset, which, in turn, can inform our data cleaning and structuring efforts as we move forward with our analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Data Cleaning/Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isna().sum())\n",
    "print(\" \")\n",
    "print(len(df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We can initiate the data cleaning process by creating an initial pipeline for our dataset. In this phase, we'll begin by removing columns that are unrelated to accidents and eliminating null values that are of minor significance (i.e., those with fewer than 180 occurrences)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_dup(df):\n",
    "    return df.drop_duplicates(subset=['Date', 'Time', 'Longitude', 'Latitude'], keep='first')\n",
    "\n",
    "def drop_drow(df):\n",
    "    return df.dropna(subset=[\"Time\", \"Pedestrian_Crossing-Human_Control\", \"Pedestrian_Crossing-Physical_Facilities\", \"Weather_Conditions\", \"Latitude\", \"Longitude\"])\n",
    "\n",
    "def drop_column(df, columns):\n",
    "    return df.drop(columns, axis=1)\n",
    "\n",
    "def fill_na(df, columns, value):\n",
    "    for column in columns:\n",
    "        df[column] = df[column].fillna(value)\n",
    "    return df\n",
    "\n",
    "def clean_pipineline(df):\n",
    "    df = remove_dup(df)\n",
    "    print(\"Number of rows after removing duplicates: \", len(df))\n",
    "    df = drop_column(df, [\"LSOA_of_Accident_Location\", 'Location_Easting_OSGR', 'Location_Northing_OSGR', 'Did_Police_Officer_Attend_Scene_of_Accident'])\n",
    "    print(\"Number of rows after dropping columns: \", len(df))\n",
    "    df = drop_drow(df)\n",
    "    print(\"Number of rows after dropping rows: \", len(df))\n",
    "    \n",
    "    df = fill_na(df, ['Special_Conditions_at_Site', 'Carriageway_Hazards'], 'No information recorded')\n",
    "    print(\"Number of rows after filling NaN values: \", len(df))\n",
    "\n",
    "    return df\n",
    "\n",
    "df = clean_pipineline(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isna().sum())\n",
    "print(\" \")\n",
    "print(len(df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " -------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Let's now conduct a more detailed examination of the column contents. To facilitate our data manipulation efforts, we will proceed by creating dedicated functions for these columns. For the more complex categorical columns, we will address missing values by using a Decision Tree model for imputation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_nulls_with_decision_tree(df, feature_cols, target_col):\n",
    "        \n",
    "    train_df = df[df[target_col].notnull()]\n",
    "    test_df = df[df[target_col].isnull()]\n",
    "\n",
    "    model = DecisionTreeClassifier(random_state=42)    \n",
    "    model.fit(train_df[feature_cols], train_df[target_col])\n",
    "    imputed_values = model.predict(test_df[feature_cols])\n",
    "    df.loc[df[target_col].isnull(), target_col] = imputed_values\n",
    "\n",
    "    return df,model\n",
    "\n",
    "def dummify(df, columns):\n",
    "    df_dummified = pd.get_dummies(df, columns=columns)\n",
    "    return df_dummified\n",
    "\n",
    "def undummify(df_dummified, column_name):\n",
    "    column_names = df_dummified.columns.tolist()\n",
    "    dummy_columns = [col for col in column_names if column_name in col]\n",
    "    df_undummified = df_dummified.copy()\n",
    "    df_undummified[column_name] = df_dummified[dummy_columns].idxmax(axis=1).str.replace(f\"^{column_name}_\", \"\")\n",
    "    df_undummified.drop(dummy_columns, axis=1, inplace=True)\n",
    "    return df_undummified\n",
    "\n",
    "def create_temporary_mapper(df, column_name):\n",
    "    unique_values = df[column_name].unique()\n",
    "    new_values = range(len(unique_values))\n",
    "    mapper = dict(zip(unique_values, new_values))\n",
    "    return mapper\n",
    "\n",
    "def display_aggregation(df, *columns):\n",
    "    H = list(columns)\n",
    "    C= list(columns)\n",
    "    C.append('Accident_Index')\n",
    "    print(C)\n",
    "    print(df[C].groupby(by=H).count().sort_values(by=[H[0]]+[\"Accident_Index\"],ascending=[True,False]).to_string())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['Date'].str.match(r'^\\d{2}/\\d{2}/\\d{4}$')]\n",
    "df = df[df['Time'].str.match(r'^\\d{2}:\\d{2}$')]\n",
    "\n",
    "df[\"int-Date\"] = df[\"Date\"].apply(lambda x: int(x[3:5]))\n",
    "df[\"int-Time\"] = df[\"Time\"].apply(lambda x: int(str(x)[0:2]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "sns.countplot(data=df, x=\"Light_Conditions\", palette=\"viridis\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " We have observed that we can transform this variable into an ordinal category, representing the degree of darkness as follows: 0 for No presence of darkness, 1 for Presence with light, and 2 for Total darkness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Darkness_Presence\"]=df[\"Light_Conditions\"].replace(['Daylight: Street light present',\n",
    "       'Darkness: Street lights present and lit',\n",
    "       'Darkness: Street lighting unknown',\n",
    "       'Darkness: Street lights present but unlit',\n",
    "       'Darkeness: No street lighting'] , [0,1,2,2,2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop([\"Light_Conditions\"],axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Exploring the action of 'wind'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "sns.countplot(data=df, x=\"Weather_Conditions\", palette=\"Set1\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Weather_Conditions\"]=df[\"Weather_Conditions\"].replace(['Raining without high winds', 'Fine without high winds',\n",
    "       'Snowing without high winds', 'Other', 'Fine with high winds',\n",
    "       'Raining with high winds', 'Fog or mist',\n",
    "       'Snowing with high winds'],[\"No High winds\",\"No High winds\",\"No High winds\",\"No High winds\",\"High winds\",\"High winds\",\"Fog\",\"High winds\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {\n",
    "    \"No High winds\": 0,\n",
    "    \"High winds\": 2,\n",
    "    \"Fog\": 0,\n",
    "    \"Unknown\":0\n",
    "}\n",
    "\n",
    "df[\"Weather_Conditions\"] = df[\"Weather_Conditions\"].replace(mapping)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now, we'll address the missing values in the \"Road_Surface_Conditions\" column by utilizing information from other related columns to predict suitable replacements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregation = df.groupby(\"Road_Surface_Conditions\").size().reset_index(name=\"Count\")\n",
    "print(aggregation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_temporary_mapper(df, column_name):\n",
    "    unique_values = df[column_name].unique()\n",
    "    mapper = dict(zip(unique_values, range(len(unique_values))))\n",
    "    return mapper\n",
    "\n",
    "road_surface_conditions_mapper = create_temporary_mapper(df, \"Road_Surface_Conditions\")\n",
    "print(road_surface_conditions_mapper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {\n",
    "    'Dry': 5,\n",
    "    'Wet/Damp': 4,\n",
    "    'Frost/Ice': 3,\n",
    "    'Snow': 2,\n",
    "    'Flood (Over 3cm of water)': 1,\n",
    "    pd.NA:pd.NA\n",
    "}\n",
    "\n",
    "df['Road_Surface_Conditions_Ordinal'] = df['Road_Surface_Conditions'].replace(mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=df, x='Road_Surface_Conditions_Ordinal', palette='Set2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = ['Urban_or_Rural_Area', 'Accident_Severity', 'Darkness_Presence', 'Weather_Conditions']\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10, 10))\n",
    "fig.suptitle('Road Surface Conditions vs. Other Variables')\n",
    "\n",
    "for i, var in enumerate(variables):\n",
    "    row = i // 2\n",
    "    col = i % 2\n",
    "    \n",
    "    sns.countplot(data=df, x='Road_Surface_Conditions_Ordinal', hue=var, ax=axs[row, col])\n",
    "    \n",
    "    axs[row, col].set_title(var)\n",
    "    axs[row, col].legend(loc='upper right')\n",
    "\n",
    "fig.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number_of_Vehicles\")\n",
    "display_aggregation(df,\"Road_Surface_Conditions_Ordinal\",\"Number_of_Vehicles\")\n",
    "print(\" \")\n",
    "print(\"Number_of_Casualties\")\n",
    "display_aggregation(df,\"Road_Surface_Conditions_Ordinal\",\"Number_of_Casualties\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Features of note: : ['Urban_or_Rural_Area', 'Accident_Severity',\"Number_of_Vehicles\",\"Number_of_Casualties\", 'Weather_Conditions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_aggregation(df,\"Road_Surface_Conditions_Ordinal\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned,clf = fill_nulls_with_decision_tree(df, ['Urban_or_Rural_Area', 'Accident_Severity',\"Number_of_Vehicles\",\"Number_of_Casualties\", 'Weather_Conditions'],'Road_Surface_Conditions_Ordinal')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_aggregation(df,\"Road_Surface_Conditions_Ordinal\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned['Road_Surface_Conditions']=df_cleaned['Road_Surface_Conditions_Ordinal']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned=df_cleaned.drop(['Road_Surface_Conditions'],axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Junction_Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_aggregation(df,\"Junction_Control\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned[\"Junction_Control\"]=df_cleaned[\"Junction_Control\"].replace([pd.NA],[\"No Junction\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned=df_cleaned.drop(['Junction_Detail'],axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " -----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Number_of_Vehicles / Number_of_Casualties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df_cleaned[\"Number_of_Vehicles\"], df_cleaned[\"Number_of_Casualties\"])\n",
    "plt.xlabel(\"Number_of_Vehicles\")\n",
    "plt.ylabel(\"Number_of_Casualties\")\n",
    "plt.title(\"Scatter Plot of Number_of_Vehicles vs. Number_of_Casualties\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(df_cleaned[\"Number_of_Vehicles\"])\n",
    "plt.xlabel(\"Number_of_Vehicles\")\n",
    "plt.title(\"Box Plot of Number_of_Vehicles\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(df_cleaned[\"Number_of_Casualties\"])\n",
    "plt.xlabel(\"Number_of_Casualties\")\n",
    "plt.title(\"Box Plot of Number_of_Casualties\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned=df_cleaned[(df_cleaned[\"Number_of_Vehicles\"]<20)&(df_cleaned[\"Number_of_Casualties\"]<45)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " -----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Speed_Limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_limit_counts = df_cleaned['Speed_limit'].value_counts()\n",
    "labels = speed_limit_counts.index\n",
    "counts = speed_limit_counts.values\n",
    "\n",
    "plt.figure(figsize=(10, 11))\n",
    "plt.pie(counts, labels=None, autopct=None, startangle=140)\n",
    "plt.title(\"Pie Chart of Speed Limits\")\n",
    "plt.axis('equal')  \n",
    "\n",
    "percentage_labels = ['{:.1f}%'.format(pct) for pct in (counts / counts.sum() * 100)]\n",
    "plt.legend(labels=[f'{label} ({percentage})' for label, percentage in zip(labels, percentage_labels)], title=\"Speed Limit\", loc=\"upper left\")\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " -----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### \"Pedestrian_Crossing-Human_Control\" & 'Pedestrian_Crossing-Physical_Facilities'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_palette = sns.color_palette(\"Set1\") \n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=df_cleaned, x=\"Pedestrian_Crossing-Human_Control\", palette=custom_palette)\n",
    "plt.title(\"Pedestrian Crossing Human Control\")\n",
    "plt.xlabel(\"Control Type\")\n",
    "plt.ylabel(\"Count\")\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned['Pedestrian_Crossing-Physical_Facilities'] = df_cleaned['Pedestrian_Crossing-Physical_Facilities'].str.replace(\"-\", \"\\n\")\n",
    "\n",
    "custom_palette = sns.color_palette(\"Set2\")  \n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.countplot(\n",
    "    data=df_cleaned,\n",
    "    x=\"Pedestrian_Crossing-Physical_Facilities\",\n",
    "    palette=custom_palette\n",
    ")\n",
    "plt.title(\"Pedestrian Crossing Physical Facilities\")\n",
    "plt.xlabel(\"Facilities Type\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " As evident, it would be more beneficial to exclude these values, particularly because our primary focus is on the accidents themselves rather than the involvement of pedestrians.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.groupby(by=\"Pedestrian_Crossing-Human_Control\").size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned=df_cleaned.drop([\"Pedestrian_Crossing-Human_Control\",'Pedestrian_Crossing-Physical_Facilities'],axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " -----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### \"Year\", \"Day_of_Week\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_palette = sns.color_palette(\"Set2\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=df_cleaned, x=\"Year\", palette=custom_palette)\n",
    "plt.title(\"Distribution of Accidents by Year\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the amount of traffic accidents has diminished over the years, displaying a downwards trend with an unexpected increase in year 2012. It is possible that this is due to a larger increase in tourism which will need to be explored seperately\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_palette = sns.color_palette(\"Set2\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=df_cleaned, x=\"Day_of_Week\")\n",
    "plt.title(\"Distribution of Accidents by Day of the Week\")\n",
    "plt.xlabel(\"Day of the Week\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_aggregation(df_cleaned,\"Day_of_Week\",\"Accident_Severity\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned=df_cleaned.drop([\"Day_of_Week\"],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_columns = set(df.columns)\n",
    "cleaned_columns = set(df_cleaned.columns)\n",
    "removed_columns = original_columns - cleaned_columns\n",
    "\n",
    "print(\"Columns that have been removed:\", removed_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " -----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### '2nd_Road_Class'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "sns.countplot(df_cleaned,hue='2nd_Road_Class',x='Accident_Severity')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "df_filtered = df_cleaned[~df_cleaned['2nd_Road_Class'].isin([-1, 6])]\n",
    "sns.countplot(data=df_filtered, x='Accident_Severity', hue='2nd_Road_Class')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " I'm skeptical about the relevance of knowing the classification of the second road in our analysis, as it doesn't appear to significantly vary with the severity of the accidents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned=df_cleaned.drop(['2nd_Road_Class','2nd_Road_Number'],axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " -----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### \"Special_Conditions_at_Site\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.groupby(by=[\"Special_Conditions_at_Site\"]).size()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " There are quite a few NANs in this section and thus it is best to remove from skewing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "filtered_df = df_cleaned[(df_cleaned[\"Special_Conditions_at_Site\"] != \"None\") &\n",
    "                         (df_cleaned[\"Special_Conditions_at_Site\"] != \"No information recorded\")]\n",
    "\n",
    "sns.countplot(data=filtered_df, x='Accident_Severity', hue='Special_Conditions_at_Site')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We want to retain this column because it's important for us to understand how accidents are influenced by this particular feature.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned=df_cleaned.drop(['Local_Authority_(District)',\n",
    "       'Local_Authority_(Highway)','Carriageway_Hazards','Accident_Index','Police_Force',\n",
    "       '1st_Road_Number'],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned['datetime'] = pd.to_datetime(df_cleaned['Date'] + ' ' + df_cleaned['Time'], format='%d/%m/%Y %H:%M')\n",
    "df_cleaned.drop([\"Date\", \"Time\", \"int-Time\", \"int-Date\"], axis=1, inplace=True)\n",
    "df_cleaned['datetime'] = df_cleaned['datetime'].dt.strftime('%Y-%m-%dT%H:%M:%S.%fZ')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Spatial Informations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uk_map = gpd.read_file(\"/Users/bashir/Documents/UCL/Quantitative Methods/GBR_adm/GBR_adm1.shp\")\n",
    "\n",
    "years = [2005, 2006, 2007, 2009, 2010, 2011, 2012, 2013, 2014]\n",
    "\n",
    "for year in years:\n",
    "    data = df_cleaned[df_cleaned[\"Year\"] == year]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    uk_map.plot(ax=ax, alpha=0.5)\n",
    "    plt.scatter(data[\"Longitude\"], data[\"Latitude\"], alpha=0.2, s=3, c='r')\n",
    "    plt.title(f\"Scatter plot for year {year}\")\n",
    "\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.density_mapbox(\n",
    "    df_cleaned[(df_cleaned[\"Year\"] >= 2005) & (df_cleaned[\"Year\"] <= 2014)],\n",
    "    lat='Latitude',\n",
    "    lon='Longitude',\n",
    "    mapbox_style=\"stamen-terrain\",\n",
    "    opacity=0.6 \n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * Looks like most accidents happen, as could be expected; in and around major cities such as London, Liverpool and the Midlands, and around Newcastle and Middlesbrough in the North East. Meanwhile in Scotland there's a big concentration as well, between Glasgow and Edinburgh. In Wales, most accidents occur near the capital, Cardiff. \n",
    " * The majority of the data is centered around the central and southern regions of the UK.\n",
    " * Data clusters are more pronounced in larger urban areas.\n",
    " * There is a noticeable decline in accidents between 2005 and 2014."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " ### NB :\n",
    "\n",
    " I encountered a significant issue due to the size of our dataset, as processing over a million rows would be extremely time-consuming. To address this, I will work with a reduced dataset by sampling more than 100,000 occurrences, aiming for approximately 22,000 samples for each year.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Sub-sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=df_cleaned\n",
    "data[\"datetime\"]=pd.to_datetime(data[\"datetime\"])\n",
    "\n",
    "data[\"Year\"]=data[\"datetime\"].apply(lambda x:x.year)\n",
    "data[\"Month\"]=data[\"datetime\"].apply(lambda x:x.month)\n",
    "\n",
    "year_month_groups = data.groupby(['Year', 'Month'])\n",
    "\n",
    "year_month_counts = year_month_groups.size()\n",
    "\n",
    "total_count = len(data)\n",
    "year_month_proportions = year_month_counts / total_count\n",
    "\n",
    "GROUP_SIZE=100000\n",
    "\n",
    "sample_sizes = np.round(year_month_proportions * GROUP_SIZE).astype(int)\n",
    "\n",
    "samples = []\n",
    "for year_month, group in year_month_groups:\n",
    "    sample = group.sample(sample_sizes[year_month])\n",
    "    samples.append(sample)\n",
    "samples = pd.concat(samples)\n",
    "\n",
    "expected_counts = year_month_proportions * len(samples)\n",
    "\n",
    "observed = year_month_counts.values.reshape(-1, 1)\n",
    "expected = expected_counts.values.reshape(-1, 1)\n",
    "contingency_table = np.concatenate([observed, expected], axis=1)\n",
    "\n",
    "chi2_stat, p_val, dof, expected_counts = chi2_contingency(contingency_table)\n",
    "\n",
    "print(\"Chi-squared test results:\")\n",
    "print(f\"  Chi-squared statistic: {chi2_stat:.2f}\")\n",
    "print(f\"  Degrees of freedom: {dof}\")\n",
    "print(f\"  p-value: {p_val:.5f}\")\n",
    "if p_val < 0.05:\n",
    "    print(\"  The proportions of samples taken from each year-month group are significantly different.\")\n",
    "else:\n",
    "    print(\"  The proportions of samples taken from each year-month group are not significantly different.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wranling_data(samples,columns_to_dummify):\n",
    "    try:\n",
    "        samples = dummify(samples.drop([\"Unnamed: 0\"],axis=1),columns_to_dummify)\n",
    "    except:\n",
    "        samples = dummify(samples,columns_to_dummify)\n",
    "    samples=samples.drop([\"datetime\"],axis=1)\n",
    "    return samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples=wranling_data(samples,[\"Road_Type\",\"Junction_Control\",\"Road_Surface_Conditions_Ordinal\",\"Special_Conditions_at_Site\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Applying 1st Clustering Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in samples[\"Year\"].unique():\n",
    "    try:\n",
    "        X = samples[samples[\"Year\"] == year].drop(\"Clusters\",axis=1)\n",
    "    except:\n",
    "        X = samples[samples[\"Year\"] == year]\n",
    "    scaler = StandardScaler()\n",
    "    X_std = scaler.fit_transform(X)\n",
    "    dbscan = DBSCAN(eps=0.8, min_samples=8)\n",
    "    clusters = dbscan.fit_predict(X_std)\n",
    "    clusters = np.where(clusters == -1, np.nan, clusters)\n",
    "    samples.loc[samples[\"Year\"] == year, \"Clusters\"] = clusters\n",
    "    samples[\"Clusters\"].fillna(-1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_one_feature_mode(df,year,cluster,column):\n",
    "    print(column,' ',df[(df[\"Year\"]==year) & (df[\"Clusters\"]==cluster)][column].unique())\n",
    "\n",
    "def print_cluster(df,year,cluster,open_plot):\n",
    "    for i in df.drop([\"Clusters\"],axis=1).columns[3:]:\n",
    "        print_one_feature_mode(df,year,cluster,i)\n",
    "    if open_plot:\n",
    "        fig, ax = plt.subplots(figsize=(8,8))\n",
    "        uk_map.plot(ax=ax, alpha=0.5)\n",
    "        plt.scatter(samples[(samples[\"Year\"]==year) & (samples[\"Clusters\"]==cluster)][\"Longitude\"], samples[(samples[\"Year\"]==year) & (samples[\"Clusters\"]==cluster)][\"Latitude\"], alpha=0.2, s=3, c='r')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_dummify=[\"Road_Type\",\"Junction_Control\",\"Road_Surface_Conditions_Ordinal\",\"Special_Conditions_at_Site\"]\n",
    "\n",
    "for i in columns_to_dummify:\n",
    "    samples=undummify(samples,i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in samples[\"Year\"].unique():\n",
    "    print(\" \")\n",
    "    print(\"** Year : \",i,\" **\")\n",
    "    for j in samples[samples[\"Year\"]==i]['Clusters'].value_counts().index[1:4]:\n",
    "        print(\"Cluster : \",j)\n",
    "        print_cluster(samples,i,j,False)\n",
    "        print(\" \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.density_mapbox(X, lat='Latitude', lon='Longitude',mapbox_style=\"stamen-terrain\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Concentrations can be seen in London, Birmingham and other visible locations here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Applying Clustering Algorithm for Special Places"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " df_test=df_cleaned[df_cleaned[\"Special_Conditions_at_Site\"]!=\"None\"]\n",
    " df_test=wranling_data(df_test,[\"Road_Type\",\"Junction_Control\",\"Road_Surface_Conditions_Ordinal\",\"Special_Conditions_at_Site\"])\n",
    " \n",
    "X=df_test\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_std = scaler.fit_transform(X)\n",
    "dbscan = DBSCAN(eps=0.8, min_samples=8)\n",
    "clusters = dbscan.fit_predict(X_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " df_test['cluster'] = clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " columns_to_dummify=[\"Road_Type\",\"Junction_Control\",\"Road_Surface_Conditions_Ordinal\",\"Special_Conditions_at_Site\"]\n",
    "\n",
    " for i in columns_to_dummify:\n",
    "     df_test=undummify(df_test,i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " for j in df_test.groupby(by=\"cluster\").size().sort_values(ascending=False).index[1:5]:\n",
    "     print(\"** CLUSTER \",j,\" **\")\n",
    "     for i in df_test.drop([\"cluster\"],axis=1).columns[3:]:\n",
    "         print(i,' : ',df_test[df_test[\"cluster\"]==j][i].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " -----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Hot Spot Analysis\n",
    " Another interesting method to use would be to look for the correlation between the spatial data, this gives a score that can be interpreted as a \"Low\" or \"High\" score and allows to have different areas as well.\n",
    " For this, we will use libraries made for spatial data analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=gpd.GeoDataFrame(df_cleaned, geometry=gpd.points_from_xy(df_cleaned.Longitude, df_cleaned.Latitude))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from libpysal.weights import Kernel\n",
    "from esda.moran import Moran\n",
    "from esda.moran import Moran_Local\n",
    "X=data[data[\"Year\"]==2014].sample(100)\n",
    "coords = X['geometry'].apply(lambda p: (p.x, p.y)).tolist()\n",
    "\n",
    "w = Kernel(coords, bandwidth=1000)\n",
    "\n",
    "moran = Moran(X['Number_of_Vehicles'], w)\n",
    "print(\"Moran's I: \", moran.I)\n",
    "\n",
    "lisa = Moran_Local(X['Number_of_Vehicles'], w)\n",
    "\n",
    "clusters = X.loc[(lisa.p_sim < 0.05) & (lisa.q==1)]\n",
    "print(clusters.head())\n",
    "\n",
    "spots = X.loc[(lisa.p_sim < 0.05) & (lisa.q==3)]\n",
    "print(spots.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "X['hot_spot'] = 0\n",
    "X.loc[clusters.index, 'hot_spot'] = 1\n",
    "X.loc[spots.index, 'hot_spot'] = -1\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "uk_map.plot(ax=ax, alpha=0.5)\n",
    "X.plot(column='hot_spot', cmap=sns.diverging_palette(255, 15, as_cmap=True), ax=ax)\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
